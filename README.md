# 🚀 LLM学习计划

## 🌟 开场白
本专栏以周为单位，记录我从前端转大模型领域的学习历程——从理论到代码，从迷茫到顿悟。感谢你的关注与陪伴！🙏

## 📅 学习规划

### 📚 第1-2个月：基础巩固与环境搭建

#### 🐍 第1-2周：Python与深度学习框架强化
- Python高级特性（装饰器、生成器、异步编程）
- PyTorch基础（张量操作、自动求导、模型定义）
- 完成一个CNN图像分类项目（利用你已有的CNN知识）

#### ➕ 第3-4周：数学基础补强
- 线性代数（矩阵分解、特征值）
- 概率统计（贝叶斯理论、分布）
- 优化理论（梯度下降、Adam优化器）

#### 🤖 第5-6周：大模型基础与环境搭建
- 学习大模型基本概念、发展历程、应用场景
- 搭建开发环境：PyTorch、CUDA、Hugging Face生态
- 体验提示词工程，熟悉主流模型API（Qwen、LLaMA）

#### 🔄 第7-8周：Transformer深度学习
- 重学Transformer架构，深入理解self-attention机制
- 实现一个简化版Transformer模型
- 阅读《Attention is All You Need》原论文

### 🧠 第3-6个月：核心技术深度学习

#### 🏗️ 第9-12周：LLM架构与预训练
- 学习BERT、GPT、T5等经典架构
- 理解预训练范式革命，掌握掩码语言模型原理
- 在小数据集上实现预训练过程

#### ✨ 第13-16周：微调技术与RAG
- 学习LoRA、P-tuning等参数高效微调技术
- 实现RAG（检索增强生成）系统
- 在你已有的AI问答平台基础上集成RAG功能

#### 🏋️ 第17-20周：大模型训练与优化
- 学习分布式训练（数据并行、模型并行）
- 模型量化、剪枝、蒸馏等优化技术
- 在单卡环境下训练1B规模的小型大模型

#### 🎯 第21-24周：多模态与Agent
- 学习CLIP、BLIP等多模态模型
- 构建AI Agent系统，结合你的前端技能做可视化
- 参与开源项目（如LangChain、LlamaIndex）

### 🚀 第7-9个月：项目实践与论文研读

#### 🎓 第25-28周：垂直领域大模型项目
- 选择一个垂直领域（如医疗、金融、教育）
- 从数据收集、清洗到模型微调全流程实践
- 构建完整的MLOps pipeline

#### 📖 第29-32周：前沿论文研读
- 每周精读1-2篇顶会论文（NeurIPS、ICLR、ACL）
- 复现论文中的关键实验
- 在GitHub上开源你的复现代码

#### ⚡ 第33-36周：性能优化与部署
- 模型推理优化（vLLM、TensorRT-LLM）
- 将模型部署到云服务（AWS SageMaker、阿里云PAI）
- 利用你的Node.js技能构建高性能推理API